{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://akademie.datamics.com/kursliste/\">![title](bg_datamics_top.png)</a>\n",
    "\n",
    "<center><em>© Datamics</em></center><br><center><em>Besuche uns für mehr Informationen auf <a href='https://akademie.datamics.com/kursliste/'>www.akademie.datamics.com</a></em></center>\n",
    "\n",
    "# Keras Grundlagen\n",
    "\n",
    "Willkommen im Abschnitt Deep Learning (tiefgehendes Lernen)! Wir werden Keras mit einem TensorFlow-Backend verwenden, um unsere Deep Learning-Operationen auszuführen.\n",
    "\n",
    "Das bedeutetd, wir sollten uns mit den Grundlagen von Keras bekannt machen.\n",
    "\n",
    "## Importe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datensatz\n",
    "\n",
    "Wir werden den Datensatz Bank Authentication Data für den Anfang verwenden. Dieser Datensatz enthält verschiedene Bild-Features (Merkmale) abgeleitet von Bildern im Format 400 x 400 Pixel. Beachte hierbei, **die von uns verwendeten Daten SIND KEINE BILDER**, sie sind **Features** (Merkmale) von Bildern. In der nächsten Lektion werden wir das Bearbeiten von Bildern mit Keras behandeln. Dieses Notebook ist fokussiert auf die Grundlagen des Erzeugens von Neuronalen Netzwerken (neural networks) mit Keras.\n",
    "\n",
    "_____\n",
    "Mehr Informationen zum Datensatz:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/banknote+authentication\n",
    "\n",
    "Die Daten wurden extrahiert von Bildern von echten und gefälschten Banknoten-artigen Objekten. Für die Digitalisierung wurde eine für die Inspektion von Druckerzeugnissen entwickelte Industriekamera verwendet. Die endgültigen Bilder haben 400 x 400 Pixel. Durch die Objektivlinse und Entfernung zum untersuchten Objekt konnten Graustufenbilder mit einer Auflösung von etwa 660 dpi gewonnen werden. Wavelet-Transformationswerkzeuge wurden verwendet, um die Features aus den Bildern zu extrahieren.\n",
    "\n",
    "\n",
    "Attributinformation:\n",
    "\n",
    "1. Varianz des transformierten Wavelet Bildes (stufenlos) \n",
    "2. Asymmetrie des transformierten Wavelet Bildes (stufenlos) \n",
    "3. Kurtosis des transformierten Wavelet Bildes (stufenlos) \n",
    "4. Entropie des Bildes (stufenlos) \n",
    "5. Klasse (Ganzzahl) \n",
    "\n",
    "## Den Datensatz einlesen\n",
    "\n",
    "Wir haben den Datensatz bereits heruntergeladen, er ist im Verzeichnis DATA. Lass ihn uns nun öffnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "data = genfromtxt('../DATA/bank_note_data.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[:,0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teile den Datensatz in Trainings- und Testdaten\n",
    "\n",
    "Es ist an der Zeit, die Daten aufzuteilen für Training und Test. Behalte im Hinterkopf, dass manche gerne in drei Teile aufteilen, Training/Test/Validierung. Vorerst werden wir bei der einfacheren Variante bleiben. **Bitte beachte die Erklärung im Video, wieso wir aufteilen und was die ganzen Parameter bedeuten!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die Daten standardisieren\n",
    "\n",
    "Bei der Anwendung Neuronaler Netzwerke ist die Leistung üblicherweise besser, wenn die Daten standardisiert werden. Standardisierung bedeutet lediglich, dass die Daten in ein bestimmtes Intervall passen, wie 0-1 oder -1 to 1.\n",
    "\n",
    "Die Scikit Learn-Bibliothek stellt auch schöne Funktionen dafür bereit.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_object = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_object.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = scaler_object.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_test = scaler_object.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, jetzt haben wir die Daten skaliert!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Das Netzwerk mit Keras aufbauen\n",
    "\n",
    "Lasst uns ein einfaches Neuronales Netzwerk bauen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeuge das Modell\n",
    "model = Sequential()\n",
    "# 8 Neuronen, erwarte Eingabe von 4 Features.\n",
    "# Spiele herum mit der Anzahl von Neuronen!!\n",
    "model.add(Dense(4, input_dim=4, activation='relu'))\n",
    "# Füge ein weiteres Densely Connected layer (dicht verbundene Schicht; jedes Neuron ist mit jedem Neuron der nächsten Schicht verbunden) hinzu\n",
    "model.add(Dense(8, activation='relu'))\n",
    "# Die letzte Schicht ist eine einfache, sigmoide Funktion zur Ausgabe von 0 oder 1 (unserer Labels [Kennzeichen])\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell kompilieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spiele auch mit der Anzahl der Epochen herum!\n",
    "model.fit(scaled_X_train,y_train,epochs=50, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorhersage neuer, ungesehener Daten\n",
    "\n",
    "Lasst uns mit **neuen Daten** überprüfen, wie gut unsere Vorhersage ist. Wie du dich erinnerst, hat unser Modell **nie** die vorher skalierten Testdaten gesehen! Dies ist genau der gleiche Prozess, den du mit komplett neuen Daten verwenden würdest. Beispielsweise eine brandneue Banknote, die du gerade analysierst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.predict(scaled_X_test) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modellleistung evaluieren\n",
    "\n",
    "Wie gut waren wir? Wie messen wir überhaupt \"gut\"? Ist 95% Treffsicherheit gut genug? Das ist alles abhängig von der Situation. Wir müssen ausserdem recall (Rückruf) und precision (Präzision) berücksichtigen. Stelle sicher, die Videodiskussion zur Klassifizierungsevaluation vor dem ausführen dieses Codes anzusehen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x=scaled_X_test,y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (model.predict(scaled_X_test) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speichern und Laden von Modellen\n",
    "\n",
    "Nachdem wir nun ein Modell trainiert haben, schauen wir uns an, wie wir es speichern und laden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('myfirstmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel = load_model('myfirstmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gut gemacht! Du weißt jetzt, wie die Daten vorzubereiten sind, wie ein Neuronales Netzwerk trainiert wird und wie die Klassifizierungsleistung beurteilt wird!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
